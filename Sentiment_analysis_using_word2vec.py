# -*- coding: utf-8 -*-
"""Rohit_Sharma_CS_584_Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wqrouZX0kxAar0XSBV72hGb101mYBKa-
"""

# Importing all the libraries
import numpy as np
import pandas as pd
import random
import scipy as sp
from sklearn.decomposition import TruncatedSVD
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
nltk.download('stopwords')
nltk.download('punkt')



import gensim.downloader as api

"""### 1. Performing Binary Classification where if overall = 4,5 the our target sentiment  = "good i.e 1" else "bad i.e 0"
### 2. Reading the data, analysing the data using Plots and summaries
### 3. The analysis shows that the data is biased since the positive setiment is 10x more than the negative sentiment (Neutral==3 was discarded in the analysis)
### 4. The data is then splitted to training, validation and test
### 5. The training dataset is used as a corpus to build a co-ocurrrence matrix and word2index hashmap
### 6. SVD was performed by using sklearn package to reduce the dimensionality of our co co-ocurrrence matrix
### 7. Plotted a scatter plot for certain words to understand the similarity using user defined function and Glove(where only two components were considerd to build the reduced co-occurence matrix in Glove)
### 8. The scatter plot plotted using the Co-occurence matrix gernerated by the user-defined function have better similarity index between words and similar words are closed compared to Glove implementation
### 9. Finally k=128 components were extracted usign SVD from the original co-occurence matrix and final embeddings were generated to be used to train the model
### 10. Final embeddibngs for both validation and test set were also generated
### 11. Logistic regression and Newural Network was implemented

### Note: After performing the validation, the best set of architecture and hyperparameters were chosen but model was not retrained on training + validation data and used directly to perform predictions on test dataset

# Task 1: Extracting features

### 1. Data Preprocessing
"""

data = pd.read_csv("D:/Stevens/CS 584/amazon_reviews.csv")

data.shape

data.head()

# FIltering out neutral reviews
data_filtered = data[data['overall'] != 3]

# Creating a target label "sentiment" conatining binary classification 0/1
data_filtered['sentiment'] = data_filtered['overall'].apply(lambda x: 1 if x in [4, 5] else 0)

data_filtered.head()

data_filtered.shape

data_filtered.isna().value_counts()

data_filtered[data_filtered["reviewText"].isna()]

data_filtered = data_filtered.dropna()

data_filtered.isna().value_counts()

"""#### Converting all texts to lower to maintain uniformity"""

for i in range(data_filtered.shape[0]):
    data_filtered.iloc[i, 1] = data_filtered.iloc[i, 1].lower()

data_filtered.head(20)

# Processing the corpus and generating tokens

def tokenize_review(review):
    # Remove punctuation
    review = review.translate(str.maketrans('', '', string.punctuation))

    # Remove numbers
    review = ''.join([i for i in review if not i.isdigit()])

    # Tokenize the review
    tokens = word_tokenize(review)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token.lower() not in stop_words]

    return tokens

data_filtered['tokenized_review'] = data_filtered['reviewText'].apply(tokenize_review)

data_filtered["tokenized_review"]

# Split the data into training (80%), validation(10%), and testing (10%)

X = data_filtered['tokenized_review']
y = data_filtered['sentiment']
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=39)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=28)

print(f"Training set size: {len(X_train)}")
print(f"Validation set size: {len(X_val)}")
print(f"Testing set size: {len(X_test)}")

X_train

for i in X_train:
    print(i)

colors_plt = ['#99ff99','#66b3ff','#ffcc99','#ff9999']
data_filtered['sentiment'].value_counts().plot.pie(autopct='%1.1f%%', shadow=True, figsize=(6,6), colors=colors_plt)
plt.show()
data_filtered["sentiment"].value_counts()*100/(data_filtered.shape[0])

colors_plt = ['#99ff99','#66b3ff','#ffcc99','#ff9999']
y_train.value_counts().plot.pie(autopct='%1.1f%%', shadow=True, figsize=(6,6), colors=colors_plt)
plt.show()
y_train.value_counts()

colors_plt = ['#99ff99','#66b3ff','#ffcc99','#ff9999']
y_val.value_counts().plot.pie(autopct='%1.1f%%', shadow=True, figsize=(6,6), colors=colors_plt)
plt.show()
y_val.value_counts()

colors_plt = ['#99ff99','#66b3ff','#ffcc99','#ff9999']
y_test.value_counts().plot.pie(autopct='%1.1f%%', shadow=True, figsize=(6,6), colors=colors_plt)
plt.show()
y_test.value_counts()

data_filtered.describe()

data_filtered.info()

data_filtered["overall"].value_counts()

max_tokens_index = X_train.apply(len).idxmax()
print(f"Maximum number of tokens:\n{X_train[max_tokens_index]}")
print(len(X_train[max_tokens_index]))

min_tokens_index = X_train.apply(len).idxmin()
print(f"Minimum number of tokens:\n{X_train[min_tokens_index]}")
print(len(X_train[min_tokens_index]))

max_tokens_index = X_val.apply(len).idxmax()
print(f"Maximum number of tokens:\n{X_val[max_tokens_index]}")
print(len(X_val[max_tokens_index]))

min_tokens_index = X_val.apply(len).idxmin()
print(f"Minimum number of tokens:\n{X_val[min_tokens_index]}")
print(len(X_val[min_tokens_index]))

max_tokens_index = X_test.apply(len).idxmax()
print(f"Maximum number of tokens:\n{X_test[max_tokens_index]}")
print(len(X_test[max_tokens_index]))

min_tokens_index = X_test.apply(len).idxmin()
print(f"Minimum number of tokens:\n{X_test[min_tokens_index]}")
print(len(X_test[min_tokens_index]))

"""## 2. Representation of Texts: word vectors (40 points)"""

# Defining a function to return set of unique words in the corpus
def get_vacab(corpus):
    corpus_words = list(set(x for i in corpus for x in i))
    corpus_words = sorted(corpus_words)
    return corpus_words

#  Defining a function to calculate the co-occurence matrix and generating hashmap
def compute_co_occurrence_matrix(corpus, window_size=4):
    word2index = {}
    corpus_words = get_vacab(corpus)

    # The below function generates a hashmap of tokens and it's corresponding index
    for index, element in enumerate(corpus_words):
        if element not in word2index:
            word2index[element] = index

    co_occurrence_matrix = np.zeros((len(corpus_words), len(corpus_words)))

    for review in corpus:
        for index, word in enumerate(review):
            # Since the corpus has varied length of tokens, we want to ensure that correct indexes
            # were used to calculate the co-occurence matrix so defining conditions to get start and end indexes for calculation
            start = max(0, index - window_size)
            end = min(len(review), index + window_size + 1)

            for word_i in range(start, end):
                if word_i != index:
                    token = review[word_i]
                    if word in word2index and token in word2index:
                        co_occurrence_matrix[word2index[word]][word2index[token]] += 1
    return co_occurrence_matrix, word2index

co_occurrence_matrix , word2index = compute_co_occurrence_matrix(X_train, window_size=4)

co_occurrence_matrix.shape

# Defining a function to reduce the dimensionality of Co-occurence matrix to K dimesnion using SVD from sklearn library
def reduce_to_k_dim(M, k):
    svd = TruncatedSVD(n_components=k, n_iter=10, random_state=89)
    M_reduced = svd.fit_transform(M)
    return M_reduced

k = 150
M_reduced = reduce_to_k_dim(co_occurrence_matrix, k)

M_reduced.shape

M_reduced[0].shape

# Defining a function to plot a scatter plot for a given set of words

def plot_embeddings(M_reduced, word2index, words_to_plot):
    indices_to_plot = [word2index[word] for word in words_to_plot]
    words_to_plot_coords = M_reduced[indices_to_plot]
    print(M_reduced[indices_to_plot].shape)

    plt.figure(figsize=(10, 8))
    plt.scatter(words_to_plot_coords[:, 0], words_to_plot_coords[:, 1], c='blue', edgecolors='k')
    for word, coords in zip(words_to_plot, words_to_plot_coords):
        plt.text(coords[0], coords[1], word)

    plt.grid(True)
    plt.show()

# Given set of words mentioned in the assignment that needs to plotted in the scatter plot
words_to_plot = ['purchase', 'buy', 'work', 'got', 'ordered', 'received', 'product', 'item', 'deal', 'use']

plot_embeddings(M_reduced, word2index, words_to_plot)

"""### 2. Prediction-based word vectors from Glove"""

# Loading the relevant models to implement Glove
def load_embedding_model():
    wv_from_bin = api.load("glove-wiki-gigaword-200")
    print("Loaded vocab size %i" % len(list(wv_from_bin.index_to_key)))
    return wv_from_bin
wv_from_bin = load_embedding_model()

def get_matrix_of_vectors(wv_from_bin, required_words):
    """ Put the GloVe vectors into a matrix M.
        Param:
            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file
        Return:
            M: numpy matrix shape (num words, 200) containing the vectors
            word2ind: dictionary mapping each word to its row number in M
    """
    import random
    words = list(wv_from_bin.index_to_key)
    print("Shuffling words ...")
    random.seed(225)
    random.shuffle(words)
    words = words[:10000]
    print("Putting %i words into word2ind and matrix M..." % len(words))
    word2ind = {}
    M = []
    curInd = 0
    for w in words:
        try:
            M.append(wv_from_bin.get_vector(w))
            word2ind[w] = curInd
            curInd += 1
        except KeyError:
            continue
    for w in required_words:
        if w in words:
            continue
        try:
            M.append(wv_from_bin.get_vector(w))
            word2ind[w] = curInd
            curInd += 1
        except KeyError:
            continue
    M = np.stack(M)
    print("Done.")

    return M, word2ind

# This function was designed to perform some testing and needs to skipped

# def get_matrix_of_vectors(wv_from_bin, required_words):
#     """ Put the GloVe vectors into a matrix M.
#         Param:
#             wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file
#         Return:
#             M: numpy matrix shape (num words, 200) containing the vectors
#             word2ind: dictionary mapping each word to its row number in M
#     """
#     word2ind = {}#{required_words[i]:i for i in range(len(required_words))}
#     M=[]
#     for i in range(len(required_words)):
#         if required_words[i]in wv_from_bin.index_to_key:
#             M.append(wv_from_bin.get_vector(required_words[i]))
#             word2ind[required_words[i]]=len(M)-1
#     M=np.stack(M)
#     return M, word2ind

M, word2ind = get_matrix_of_vectors(wv_from_bin,get_vacab(X_train))

M.shape

k = 2
M_reduced_1 = reduce_to_k_dim(M, k)

plot_embeddings(M_reduced_1, word2ind, words_to_plot)

"""#### 2) d. Observations between plots

##### 1. Since we are reducing the k dimensions to 2 in Glove the words are separated more in the scatter plot compared to the previous scatter plot

### Task 2: Sentiment Classification Algorithms
#### Perform sentiment analysis with classification
"""

k = 128
M_reduced_2 = reduce_to_k_dim(co_occurrence_matrix, k)

review_embeddings = []
# Generating final emebeddings of the training dataset that can be fed to the model
for review in X_train:
    # Getting indices of words in the review
    word_indices = [word2index[word] for word in review if word in word2index]

    if not word_indices:
        continue

    # Get word embeddings for the review
    review_word_embeddings = M_reduced_2[word_indices]

    # Averaging the embedding vector for all the embedings in a datapoint
    review_embedding = np.mean(review_word_embeddings, axis=0)

    review_embeddings.append(review_embedding)

final_embeddings_train  = np.stack(review_embeddings)

final_embeddings_train.shape

y_train

review_embeddings = []

for review in X_val:
    word_indices = [word2index[word] for word in review if word in word2index]

    if not word_indices:
        continue

    review_word_embeddings = M_reduced_2[word_indices]
    review_embedding = np.mean(review_word_embeddings, axis=0)
    review_embeddings.append(review_embedding)
final_embeddings_val  = np.stack(review_embeddings)

final_embeddings_val.shape

review_embeddings = []

for review in X_test:
    word_indices = [word2index[word] for word in review if word in word2index]

    if not word_indices:
        continue

    review_word_embeddings = M_reduced_2[word_indices]
    review_embedding = np.mean(review_word_embeddings, axis=0)
    review_embeddings.append(review_embedding)
final_embeddings_test  = np.stack(review_embeddings)

final_embeddings_test.shape

import warnings
warnings.filterwarnings("ignore")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report,roc_auc_score,confusion_matrix, plot_confusion_matrix

# The hyperparameter C that controls the strength of regularization is tuned and the best model is selected

for C in [0.1, 0.3, 0.5, 0.7 ,1.0]:
    model1 = LogisticRegression(penalty='l2', C=C, max_iter=1000)
    model1.fit(final_embeddings_train, y_train)

    y_val_pred = model1.predict(final_embeddings_val)

    # Print reports for each C value including but not limited to accuracy, classification report, auc and cnfusion matrix
    print(f"\nReport for C={C}")
    print("Validation Accuracy:", accuracy_score(y_val, y_val_pred))
    print("Validation AUC:", roc_auc_score(y_val, y_val_pred))
    print("Validation Classification Report:\n", classification_report(y_val, y_val_pred))
    conf_matrix = confusion_matrix(y_val, y_val_pred)
    plot_confusion_matrix(model1, final_embeddings_val, y_val, cmap='Blues')
    plt.title(f'Confusion Matrix for C={C}')
    plt.show()

# Final model is trained using C=0.3 and training dataset only which generates the best results among all hyperparameters
model2 = LogisticRegression(penalty='l2', C=0.3, max_iter=1000)
model2.fit(final_embeddings_train, y_train)
y_test_pred = model2.predict(final_embeddings_test)
print("Test Accuracy:", accuracy_score(y_test, y_test_pred))
print("Test AUC:", roc_auc_score(y_test, y_test_pred))
print("Test Classification Report:\n", classification_report(y_test, y_test_pred))
conf_matrix = confusion_matrix(y_test, y_test_pred)
plot_confusion_matrix(model2, final_embeddings_test, y_test, cmap='Blues')
plt.title(f'Confusion Matrix for C={C}')
plt.show()

import tensorflow as tf
import seaborn as sns


# Build the sequential Nueral Net model
model3 = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(128,)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model3.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
model3.fit(final_embeddings_train, y_train, epochs=5, batch_size=32, validation_data=(final_embeddings_val, y_val))

# Evaluate the model
y_pred = (model3.predict(final_embeddings_val) > 0.5).astype("int32")
print("Validation Accuracy:", accuracy_score(y_val, y_pred))
print("Validation AUC:", roc_auc_score(y_val, y_pred))
print(classification_report(y_val, y_pred))
conf_matrix = confusion_matrix(y_val, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

import tensorflow as tf

model4 = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(128,)),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.7),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model4.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
model4.fit(final_embeddings_train, y_train, epochs=10, batch_size=64, validation_data=(final_embeddings_val, y_val))  # Increase epochs and batch size

# Evaluate the model
y_pred = (model4.predict(final_embeddings_val) > 0.5).astype("int32")
print("Validation Accuracy:", accuracy_score(y_val, y_pred))
print("Validation AUC:", roc_auc_score(y_val, y_pred))
print(classification_report(y_val, y_pred))
conf_matrix = confusion_matrix(y_val, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

y_pred = (model4.predict(final_embeddings_test) > 0.5).astype("int32")
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("Test AUC:", roc_auc_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""### Observations based on the results obtained above

##### 1. In the Logistic regression performed, when C=0.3, we are getting the best accuracy, AUC and F1 score.
##### 2. Two differnt architecture of Neural Net was trained having different dropout rates to regularize the neural network
##### 3. The second architecture with higher dropout rate performs better
##### 4. Among Logistic Regression and Neural Network, Logistic Regression performs better since it has higher AUC and better F1 score than Neural Network for both the classes even though the accuracy of both the models is similar


"""